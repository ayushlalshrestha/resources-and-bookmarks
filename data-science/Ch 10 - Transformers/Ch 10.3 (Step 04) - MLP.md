# Multi-layer Perceptron in GPT Architecture

- MLP adds stored (general) Knowledge
- 1st up projection Matrice asks various questions, each row asks 1 question to the embedding vector (50,000 questions/rows in GPT-3)
- Then passed through a Relu.
- Then through 2nd (down) projection matrices (it also asks questions where (now) each column asks questions.
- The resulting vector is added to the input embedding from attention layer.
- It is true that the rows of the first matrix can be thought of as directions in this embedding space, & that means the activation of each neuron tells you how much a given vector aligns with some specific direction.
- Itâ€™s also true that the columns of that 2nd matrix tell you what will be added to the result if that neuron is active.
- However, the evidence does suggest that individual neurons very rarely represent a single clean feature like "Michael Jordan", and there may actually be a very good reason this is the case, related to an idea floating around interpretability researchers these days known as superposition.
- sparse autoencoder

Notes: GPT-3 has 96 Layers, each Layer has 96 attention heads & 1 MLP