# Transformers 2

## Architecture of Transformer

Word-Embedding: Vector representation of each word in the input sequence by passing it through a pre-trained neural network. Note that while neural networks are indeed involved in the creation of word embeddings, it's more accurate to say that word embeddings are learned from large text corpora using neural network-based models rather than directly passing words through a pre-trained neural network.

Positional Encoding: Keeping track of the order of the words. Passing each word embedding through sin and cosine regarding positions

Masked Self-attention: mechanism to correctly associate the words with each other. K Q V. After this step, the self-attention values for each word is generated. For example, in the sentence "The pizza came out of the oven and it tasted good" if you looked at a lot of sentences about pizza, the word it was more commonly associated with pizza than over.

Residual connection: Now, we take the positional encoded values to the and add them to the self attention values.

Encoder-decoder attention is to allow the decoder to keep track of the significant words in the input.

GPT can be thought of as navigating a high-dimensional space (e.g., 12288 for GPT-3), where each word or token shifts its position based on context, relationships, and positional information. Instead of a physical pointer moving, its representation vector evolves dynamically as new words are added. Self-attention allows each token to adjust based on relevant past words, while an MLP further refines its position. At each step, the model predicts the next token based on where it "lands" in this space, generating text sequentially. Cached attention ensures efficiency by avoiding redundant recomputation. This process allows GPT to model language fluently by progressively refining its understanding with each new token.

## Flow

Certainly! Let's break down how an input query passes through each component of a transformer model like BERT or GPT:

1. **Tokenization**:
   - The input query is first tokenized into individual tokens or subwords using a tokenizer specific to the transformer model.
   - For example, the sentence "How are you?" might be tokenized into ["How", "are", "you", "?"].

2. **Positional Encoding**:
   - Each token is augmented with positional encoding to convey its position in the input sequence.
   - Positional encoding is typically achieved by adding sinusoidal functions of different frequencies and phases to each token embedding.

3. **Input Embedding**:
   - The tokenized input is mapped to dense vector representations known as embeddings.
   - Each token is embedded into a high-dimensional vector space, capturing semantic and contextual information.

4. **Attention Mechanism**:
   - The input embeddings, along with their positional encodings, are fed into multiple layers of attention mechanisms.
   - The attention mechanism allows the model to focus on different parts of the input sequence when processing it.
   - Self-attention is computed within each layer, enabling the model to capture dependencies between different tokens in the sequence.

5. **Feedforward Neural Networks**:
   - After attention mechanisms, the processed embeddings pass through feedforward neural networks (FFNNs) within each layer.
   - FFNNs introduce non-linearity and perform transformations on the token embeddings.

6. **Layer Normalization**:
   - Layer normalization is applied after each layer to stabilize training and improve convergence.
   - It normalizes the activations of each layer, ensuring that they have zero mean and unit variance.

7. **Output Layer**:
   - After passing through multiple layers of attention and FFNNs, the final token embeddings are fed into the output layer.
   - The output layer can vary depending on the specific task the transformer is designed for (e.g., classification, generation, translation).
   - For classification tasks, the output layer typically consists of a softmax layer for predicting class probabilities.
   - For generation tasks, the output layer generates probabilities for the next token in the sequence.

8. **Loss Computation**:
   - During training, the model's predictions are compared to the ground truth using a loss function (e.g., cross-entropy loss for classification tasks).
   - The loss is computed based on the disparity between the predicted output and the actual target output.

9. **Backpropagation and Optimization**:
   - The loss is backpropagated through the network, and the model's parameters are updated using optimization techniques like gradient descent.
   - The model learns to adjust its parameters to minimize the loss and improve its performance on the given task.

This process repeats iteratively during training until the model converges to an optimal set of parameters. During inference, the trained model can generate predictions or outputs for new input queries by passing them through the same sequence of components.

## Questions

1. Why multiple masked self attention cells?
2. Why is there a residual connection (addition) after the self attention layer?
3. Is the fully connected layer in a Decoder only transformer only of a single hidden layer?
4. What is the purpose of having multiple attention heads and also multiple layers?
