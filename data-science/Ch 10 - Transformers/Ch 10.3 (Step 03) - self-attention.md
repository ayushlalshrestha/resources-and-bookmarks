# Self-Attention Mechanism in GPT Architecture

## Overview
Self-attention is a key mechanism in Transformer models, including GPT, enabling them to weigh different words in a sequence based on their relevance to each other. It allows the model to capture dependencies regardless of distance, making it superior to traditional recurrent and convolutional networks for NLP tasks.

---

## Key Concepts: Query, Key, and Value (Q, K, V)

Each word (token) in the input sequence is transformed into three vectors:
- **Query (Q)**: Represents what this word is looking for in other words.
- **Key (K)**: Represents the properties of the word that might match a query.
- **Value (V)**: Contains the actual information to be passed to the next layer.

These vectors are computed as follows:

$$ Q = XW_Q, \quad K = XW_K, \quad V = XW_V $$

where:
- **X** is the word embedding (input representation of the token).
- **W_Q, W_K, W_V** are learned weight matrices that project the embeddings into the query, key, and value spaces.

---

## Self-Attention Calculation

1. **Compute Attention Scores**:
   - The similarity between words is determined by taking the dot product of each query with all keys:

     $$ \text{Attention Score} = QK^T $$

   - This results in an attention matrix where each entry determines the influence of one word on another.

2. **Scaling the Scores**:
   - To prevent large values from dominating the softmax output, we scale the dot product by the square root of the key dimension:

     $$ \frac{QK^T}{\sqrt{d_k}} $$

   - Where **d_k** is the dimensionality of the key vectors.

3. **Softmax Normalization**:
   - Apply softmax to get attention weights, ensuring they sum to 1:

     $$ \text{Attention Weights} = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) $$

4. **Weighted Sum of Values**:
   - Multiply the attention weights with the value vectors:

     $$ \text{Output} = \text{Attention Weights} \times V $$

   - This produces a new representation for each word based on contextual relationships in the sequence.

---

## Multi-Head Self-Attention

Instead of a single set of Q, K, V, transformers use multiple attention heads:

1. **Split input into multiple heads**:
   - Each head learns different aspects of relationships between words.

2. **Apply self-attention to each head independently**:
   - Each head computes its own set of Q, K, V and follows the same process described above.

3. **Concatenate the outputs**:
   - The outputs from all heads are concatenated and projected back into the original input dimension.

   $$ \text{MultiHead} (X) = \text{Concat} (\text{head}_1, ..., \text{head}_h) W_O $$

   - Where **W_O** is a learned weight matrix that combines the information from different heads.

In GPT-3, there are **96 layers with 96 attention heads**, and each K, Q, V vector has **128 dimensions**.

---

## Masked Self-Attention in GPT

Since GPT is an autoregressive model, it only attends to previous words and never looks ahead. This is enforced using **causal masking**:
- The upper triangular part of the attention matrix is masked out, ensuring that each token only has access to preceding tokens.

This prevents the model from using future context when generating text.

---

## Benefits of Self-Attention in GPT

- **Captures Long-Range Dependencies**: Unlike RNNs, attention does not degrade for distant tokens.
- **Parallelizable**: Unlike sequential models, attention allows parallel processing.
- **Contextual Word Representations**: Each word dynamically adjusts its meaning based on context.
- **Efficient Information Flow**: Every token can attend to all others, allowing better learning of relationships.

---

## Example Calculation

Consider the sentence: 

> *The cat sat on the mat.*

For the word "sat":
1. **Query**: Represents the properties of "sat" (e.g., what it relates to).
2. **Keys**: Represent features of all words ("The", "cat", "sat", "on", "the", "mat").
3. **Dot product of Query(sat) with Keys** assigns higher attention scores to "cat" and "mat" due to their relevance.
4. **Softmax normalizes scores** so "cat" and "mat" influence "sat" the most.
5. **Values are weighted** accordingly to update the representation of "sat".

---

## Summary
- Self-attention allows each token to dynamically relate to others.
- Queries, Keys, and Values determine how words influence each other.
- Multi-head attention helps the model learn multiple relationships.
- GPT uses **masked self-attention** to enforce autoregressive behavior.

This mechanism is the core of how GPT understands and generates language efficiently.


---
### **1. Understanding K, Q, and V in Self-Attention**
Each token (word or subword) in the input sequence is first **embedded into a high-dimensional space**. These embeddings are then transformed into three different representations:

1. **Query (Q)**:  
   - Represents the **current token** asking:  
     **"Which tokens in the sentence are relevant to me?"**
   - It is used to compare the current token to all other tokens.
  
2. **Key (K)**:  
   - Represents each tokenâ€™s **identity** in relation to others:  
     **"How important am I to other tokens?"**
   - It is used to determine if other tokens should pay attention to this token.

3. **Value (V)**:  
   - Represents the **actual information** contained in the token:  
     **"If a token attends to me, what information should I pass along?"**
   - It determines what gets added to the final representation of the word.

---

