# How does the Transformer work

## Embedding

1. Word embeddings:
    - Numerical Vector representation is generated for each input word through a "pre trained" neural network.
    - Each token (word) of the input sentence is converted into it's vector (numerical value, embedding).
    - Words with similar meaning tend to have the same vector values if you think about the representation in a space of co-ordinates.
    - In GPT-3, each vector is of 12288 dimensions.
    - The directions in the space has a semantic meaning.
    - For eg. if you subtract the embedding vector of man and woman, it will be similar to that of the difference between king and queen, so the difference represents a vector of `gender`. That's just one example, you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a words meaning.
    - The aim of a transformer is to progressively adjust these embeddings so that they don't merely encode individual word, but instead they bake in some much, much richer contextual meaning.
    - Dot products give us the similarities between 2 vectors.

## Vector Representation

- Vector Similarity: The dot product between vectors gives us an idea of how similar the words are
- Examples
    * leaves - leaf → people - person → plur 
    * plur + dog → dogs
    * plur • dog = -ve
    * plur • dogs = +ve
- Context Awareness: Think of the vector of each token as holding the capacity to soak in various contexts as it flows through the various layers in the network.
- Context Size
  - The network can only process a fixed number of tokens at a time.
  - Example: 2048 for GPT-3.
- Unembedding Layer: The last matrix (unembedding vector) distributes the "last vector" to the probabilities of each word in the vocabulary.


## Other notes

- Softmax Function
  - Used for probability distribution.
  - Softmax with **temperature** affects distribution.

- Terminologies
  - **Logits** = Input vector
